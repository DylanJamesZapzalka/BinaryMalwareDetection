from models import APTGNN
import tensorflow as tf
from datasets import APTDataSet
import gc
import numpy as np
from spektral.data import DisjointLoader



# Used to prevent OOM errors
gpus = tf.config.experimental.list_physical_devices('GPU')
tf.config.experimental.set_memory_growth(gpus[0], True)

# Hyperparameters
LEARNING_RATE = 0.0001
EPOCHS = 32



# Create and compile the model
model = APTGNN()
model.compile(
    loss=tf.keras.losses.CategoricalCrossentropy(),
    optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),
    metrics=["accuracy"]
)



"""
This way of training is very sketchy and shouldn't be implemented in this fashion.
However, due to not being able to store the node feature vectors as sparse vectors,
I will have to train the dataset through batches of size 1. I will also have to use the
disjoint loader as it is the only loader that loads the adjacency matrix as a sparse
matrix. Dense matrices are not possible as many of them would take up well over a 100GB
of memory.
To be able to train the model normally, I will have to implement some features that will
allow the loaders to use both sparse adjecency matrices and sparse node feature vectors.
This should reduce the overall both storage and memory usage by well over 99%.
"""

# Method to train the model for 1 epoch
def train(indices):

    # Used to keep track of progress and accuracy
    count = 0
    accuracy = 0

    # Perform gradient descent over the gradient given each training example
    for index in indices:

        # Get the loader for a single example
        batch = [index] 
        dataset = APTDataSet(batch)
        loader = DisjointLoader(dataset)

        # Perform gradient descent for the single example
        history = model.fit(loader.load(), steps_per_epoch=loader.steps_per_epoch, epochs=1, verbose=0)
        accuracy += history.history['accuracy'][0]

        # Delete unused memory
        del dataset
        del loader
        gc.collect()
        tf.keras.backend.clear_session()

        print(str(count))

        # Keep track of progress
        count += 1
        if count % 500 == 0:
            print("Processed: " + str(count))
    
    return accuracy / len(indices)



# Method to test the model
def test(indices):

    # Used to keep track of accuracy
    accuracy = 0
    correct_labels = np.zeros(10)
    total_labels = np.zeros(10)

    # Perform gradient descent over the gradient given each training example
    for index in indices:

        # Get the loader for a single example
        batch = [index]
        dataset = APTDataSet(batch)
        loader = DisjointLoader(dataset)

        # Perform gradient descent for the single example
        loss = model.evaluate(loader.load(), steps=loader.steps_per_epoch)
        accuracy += loss[1]
        label = [np.where(dataset.graphs[0].y == 1)[0][0]]
        correct_labels[label] = correct_labels[label] + loss[1]
        total_labels[label] = total_labels[label] + 1

        # Delete unused memory
        del dataset
        del loader
        gc.collect()
        tf.keras.backend.clear_session()
    
    return accuracy / len(indices), correct_labels, total_labels



# Get the training set
indices = [i for i in range(2609)]
indices.remove(811)
indices.remove(910)
indices.remove(846)
indices.remove(2226)

np.random.shuffle(indices)
training_set = indices[0 : int(len(indices)*.8)]
testing_set = indices[int(len(indices)*.8) : len(indices)]

# Train the data
for epoch in range(EPOCHS):
    print("Epoch: " + str(epoch))
    accuracy = train(training_set)
    print("Training Accuracy: " + str(accuracy))

# Test the data
accuracy, correct_labels, total_labels = test(testing_set)
print("Testing Accuracy: " + str(accuracy))
print(correct_labels)
print(total_labels)