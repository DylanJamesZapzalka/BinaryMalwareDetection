import angr
from angrutils import *
import networkx as nx
from networkx.algorithms.centrality.betweenness import betweenness_centrality
import numpy as np
import os



# User defined variables for processing data
file_family = 0
directory_name = "../project/data/Labeled-Elfs/group2/"
directory = os.fsencode(directory_name)
save_file_location = "../project/data/bengin_22"



# Used to keep track of how many binaries have been processed
processed = 0
total = len(os.listdir(directory))

# Obtain the control flow graphs from the binaries
graphs = []
print("CFG extraction has started...")
for file in os.listdir(directory):
    filename = os.fsdecode(file)
    try:
        proj = angr.Project(directory_name + filename, load_options={'auto_load_libs': False})
        print("Current File: " + filename)
        cfg = proj.analyses.CFGFast()
        graphs.append(cfg.graph)
        processed += 1
        print(cfg.graph)
        print(str((processed/total)*100) + "% " + "done")
    except:
        print("Failed to process file")
        processed += 1
        continue



# Obtain the features from the control flow graphs
print("Feature extraction has started...")
features = np.zeros((len(graphs), 24))
num_graphs = len(graphs)
processed = 0
for i in range(len(graphs)):
    # Calculate the number of nodes, number of edges, and the density of each graph
    features[i][0] = graphs[i].number_of_nodes()
    features[i][1] = graphs[i].number_of_edges()
    features[i][2] = nx.density(graphs[i])

    # Calculate the measurements of closeness centrality for each graph
    closeness_values = np.fromiter(nx.closeness_centrality(graphs[i]).values(), dtype=float)
    features[i][3] = np.min(closeness_values)
    features[i][4] = np.max(closeness_values)
    features[i][5] = np.median(closeness_values)
    features[i][6] = np.mean(closeness_values)
    features[i][7] = np.std(closeness_values)

    # Calculate the measurements of betweenness centrality for each graph
    betweenness_values = np.fromiter(nx.betweenness_centrality(graphs[i]).values(), dtype=float)
    features[i][8] = np.min(betweenness_values)
    features[i][9] = np.max(betweenness_values)
    features[i][10] = np.median(betweenness_values)
    features[i][11] = np.mean(betweenness_values)
    features[i][12] = np.std(betweenness_values)

    # Calculate the measurements of degree centrality for each graph
    degree_values = np.fromiter(nx.degree_centrality(graphs[i]).values(), dtype=float)
    features[i][13] = np.min(degree_values)
    features[i][14] = np.max(degree_values)
    features[i][15] = np.median(degree_values)
    features[i][16] = np.mean(degree_values)
    features[i][17] = np.std(degree_values)

    # Calculate the measurements of shortest paths for each graph
    path_values = list(nx.shortest_path_length(graphs[i]))
    path_values = [np.mean(np.array(list(j[1].values()))) for j in path_values]
    features[i][18] = np.min(path_values)
    features[i][19] = np.max(path_values)
    features[i][20] = np.median(path_values)
    features[i][21] = np.mean(path_values)
    features[i][22] = np.std(path_values)

    features[i][23] = file_family

    # Indicate how many graphs have been processed
    processed += 1
    print(str((processed/num_graphs)*100) + "% " + "done")



# Create or update the file with the training data
print("Writing features to text file...")
file = open(save_file_location, "w")
for i in range(len(features)):
    for j in range(len(features[i])):
        file.write(str(features[i][j]) + " ")
    file.write("\n")
file.close()

print("Finished!")
